{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C0Akgb_na7NU",
        "jEe2R0yAoifa",
        "FJybpt_UvCU0",
        "AgMT_U8gvM7F",
        "jRG4cBnKvZU2",
        "K6_icFWdauSI",
        "_Btki9jRvc1Y",
        "6FEsxCdvkg2D",
        "znXNx2bgUvtd",
        "zxxWSX26jsGT",
        "23z3A3f1ol6d",
        "57cQ8dzJX0Lp",
        "Wvhy6ENJf_oS",
        "26iwXg2a75qo",
        "bwOJ36hchWiN",
        "XtgWbvJsEwMt",
        "7rL4KXKM-KRC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafsanJany-44/ARC/blob/master/Activity_Recognition_EEG_2022_12_21_(edited).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zdFOS9nFlpsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8f06d3-960b-4130-a3f0-b85b2c1e6e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def models_check_box(models):\n",
        "  import ipywidgets as widgets\n",
        "  from IPython.display import display\n",
        "  new_keys=[]\n",
        "  for i in models:\n",
        "    i=widgets.Checkbox(\n",
        "      value=False,\n",
        "      description=str(i),\n",
        "      disabled=False,\n",
        "      indent=False\n",
        "      )\n",
        "    display(i)\n",
        "    new_keys.append(i)\n",
        "  return new_keys\n",
        "\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def balance(X_temp, y_temp):\n",
        "  smote = SMOTE()\n",
        "  X_temp, y_temp= smote.fit_resample(X_temp, y_temp)\n",
        "  return pd.concat([pd.DataFrame(X_temp), pd.DataFrame(y_temp)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6TIqVlzwH_z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#dataset=pd.read_csv(\"/content/drive/MyDrive/EEG_CNU_Activity Recognition/EEG_CNU_Resting, walking, working and Reading_Control_2022.12.05.csv\")\n",
        " \n",
        "dataset=pd.read_excel(\"/content/drive/MyDrive/Iqram Sir/EEG_CNU_Resting, walking, working and Reading_Control_2022.12.22.xlsx\")\n",
        "\n",
        "classes = np.array(sorted(list(set(dataset.iloc[:, 1]))))\n",
        "target = \"Activity\"\n",
        "result = {}"
      ],
      "metadata": {
        "id": "M0AmIUwSmLfj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "4YN_C8yxm1ho",
        "outputId": "2a49c3ee-9401-4bb9-b93c-b8951260e8e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Activity   Status  Epoch  MeanP_Alpha_Fz  MedianF_Alpha_Fz  MeanF_Alpha_Fz  \\\n",
              "0  Resting  Control      0        0.000008          11.70732        28.29268   \n",
              "1  Resting  Control      1        0.000010          10.73171        28.78049   \n",
              "2  Resting  Control      2        0.000008          13.17073        28.78049   \n",
              "3  Resting  Control      3        0.000011          10.24390        27.80488   \n",
              "4  Resting  Control      4        0.000007          10.73171        23.41463   \n",
              "\n",
              "   Spectral Edge_Alpha_Fz  PeakF_Alpha_Fz  MeanP_Beta_Fz  MedianF_Beta_Fz  \\\n",
              "0                20.00000        10.73171       0.000017         19.51220   \n",
              "1                22.43902        10.24390       0.000023         22.92683   \n",
              "2                23.41463         9.26829       0.000023         22.43902   \n",
              "3                18.53659        10.73171       0.000019         22.43902   \n",
              "4                18.04878        11.70732       0.000010         17.56098   \n",
              "\n",
              "   ...  Relative Power_Alpha_Global  Relative Power_Beta_Global  \\\n",
              "0  ...                     0.358154                    0.639771   \n",
              "1  ...                     0.457695                    0.806208   \n",
              "2  ...                     0.501419                    1.059244   \n",
              "3  ...                     0.497909                    0.710690   \n",
              "4  ...                     0.429825                    0.569204   \n",
              "\n",
              "   Relative Power_Theta_Global  Relative Power_Delta_Global  \\\n",
              "0                     0.749628                     4.031903   \n",
              "1                     0.723662                     3.759249   \n",
              "2                     0.737332                     3.320235   \n",
              "3                     0.907816                     3.653423   \n",
              "4                     0.849345                     4.001597   \n",
              "\n",
              "   Relative Power_Gamma_Global  ∆Relative Power_Alpha_Global  \\\n",
              "0                     0.220545                     -0.273666   \n",
              "1                     0.253186                     -0.071797   \n",
              "2                     0.381769                      0.016875   \n",
              "3                     0.230162                      0.009757   \n",
              "4                     0.150029                     -0.128318   \n",
              "\n",
              "   ∆Relative Power_Beta_Global  ∆Relative Power_Theta_Global  \\\n",
              "0                    -0.164639                     -0.009611   \n",
              "1                     0.052680                     -0.043916   \n",
              "2                     0.383074                     -0.025855   \n",
              "3                    -0.072039                      0.199384   \n",
              "4                    -0.256780                      0.122134   \n",
              "\n",
              "   ∆Relative Power_Delta_Global  ∆Relative Power_Gamma_Global  \n",
              "0                      0.107193                     -0.356230  \n",
              "1                      0.032320                     -0.260950  \n",
              "2                     -0.088237                      0.114385  \n",
              "3                      0.003259                     -0.328157  \n",
              "4                      0.098870                     -0.562065  \n",
              "\n",
              "[5 rows x 259 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7890ffa-9d02-4a9a-9444-83430ef2919a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Activity</th>\n",
              "      <th>Status</th>\n",
              "      <th>Epoch</th>\n",
              "      <th>MeanP_Alpha_Fz</th>\n",
              "      <th>MedianF_Alpha_Fz</th>\n",
              "      <th>MeanF_Alpha_Fz</th>\n",
              "      <th>Spectral Edge_Alpha_Fz</th>\n",
              "      <th>PeakF_Alpha_Fz</th>\n",
              "      <th>MeanP_Beta_Fz</th>\n",
              "      <th>MedianF_Beta_Fz</th>\n",
              "      <th>...</th>\n",
              "      <th>Relative Power_Alpha_Global</th>\n",
              "      <th>Relative Power_Beta_Global</th>\n",
              "      <th>Relative Power_Theta_Global</th>\n",
              "      <th>Relative Power_Delta_Global</th>\n",
              "      <th>Relative Power_Gamma_Global</th>\n",
              "      <th>∆Relative Power_Alpha_Global</th>\n",
              "      <th>∆Relative Power_Beta_Global</th>\n",
              "      <th>∆Relative Power_Theta_Global</th>\n",
              "      <th>∆Relative Power_Delta_Global</th>\n",
              "      <th>∆Relative Power_Gamma_Global</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Resting</td>\n",
              "      <td>Control</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>11.70732</td>\n",
              "      <td>28.29268</td>\n",
              "      <td>20.00000</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>19.51220</td>\n",
              "      <td>...</td>\n",
              "      <td>0.358154</td>\n",
              "      <td>0.639771</td>\n",
              "      <td>0.749628</td>\n",
              "      <td>4.031903</td>\n",
              "      <td>0.220545</td>\n",
              "      <td>-0.273666</td>\n",
              "      <td>-0.164639</td>\n",
              "      <td>-0.009611</td>\n",
              "      <td>0.107193</td>\n",
              "      <td>-0.356230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Resting</td>\n",
              "      <td>Control</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>28.78049</td>\n",
              "      <td>22.43902</td>\n",
              "      <td>10.24390</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>22.92683</td>\n",
              "      <td>...</td>\n",
              "      <td>0.457695</td>\n",
              "      <td>0.806208</td>\n",
              "      <td>0.723662</td>\n",
              "      <td>3.759249</td>\n",
              "      <td>0.253186</td>\n",
              "      <td>-0.071797</td>\n",
              "      <td>0.052680</td>\n",
              "      <td>-0.043916</td>\n",
              "      <td>0.032320</td>\n",
              "      <td>-0.260950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Resting</td>\n",
              "      <td>Control</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>13.17073</td>\n",
              "      <td>28.78049</td>\n",
              "      <td>23.41463</td>\n",
              "      <td>9.26829</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>22.43902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.501419</td>\n",
              "      <td>1.059244</td>\n",
              "      <td>0.737332</td>\n",
              "      <td>3.320235</td>\n",
              "      <td>0.381769</td>\n",
              "      <td>0.016875</td>\n",
              "      <td>0.383074</td>\n",
              "      <td>-0.025855</td>\n",
              "      <td>-0.088237</td>\n",
              "      <td>0.114385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Resting</td>\n",
              "      <td>Control</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>10.24390</td>\n",
              "      <td>27.80488</td>\n",
              "      <td>18.53659</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>22.43902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497909</td>\n",
              "      <td>0.710690</td>\n",
              "      <td>0.907816</td>\n",
              "      <td>3.653423</td>\n",
              "      <td>0.230162</td>\n",
              "      <td>0.009757</td>\n",
              "      <td>-0.072039</td>\n",
              "      <td>0.199384</td>\n",
              "      <td>0.003259</td>\n",
              "      <td>-0.328157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Resting</td>\n",
              "      <td>Control</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>23.41463</td>\n",
              "      <td>18.04878</td>\n",
              "      <td>11.70732</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>...</td>\n",
              "      <td>0.429825</td>\n",
              "      <td>0.569204</td>\n",
              "      <td>0.849345</td>\n",
              "      <td>4.001597</td>\n",
              "      <td>0.150029</td>\n",
              "      <td>-0.128318</td>\n",
              "      <td>-0.256780</td>\n",
              "      <td>0.122134</td>\n",
              "      <td>0.098870</td>\n",
              "      <td>-0.562065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 259 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7890ffa-9d02-4a9a-9444-83430ef2919a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7890ffa-9d02-4a9a-9444-83430ef2919a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7890ffa-9d02-4a9a-9444-83430ef2919a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8Wm0DGf4AJO",
        "outputId": "55c2f27a-861e-467f-ad43-2d4565360b0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1711, 259)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[target].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGAp91QUwrWo",
        "outputId": "83251f56-fa3e-4291-daec-b1d71c2785b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Reading    793\n",
              "Walking    408\n",
              "Working    267\n",
              "Resting    243\n",
              "Name: Activity, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(list(dataset[target]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS4iUbnyYaDH",
        "outputId": "4bbffe5f-b5a9-455e-fa77-4fb901339e94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Reading', 'Resting', 'Walking', 'Working'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder=LabelEncoder()\n",
        "dataset[target]=encoder.fit_transform(dataset[target])"
      ],
      "metadata": {
        "id": "Ljyj9yuIf-tX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(list(dataset['Activity']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42si00q3SLh5",
        "outputId": "67e8e860-e613-4b64-bdef-ffad6795eec4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spliting into X and y"
      ],
      "metadata": {
        "id": "ZAUQcyHz4wlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X =  dataset.loc[:,dataset.columns != target]  # removing Activity \n",
        "X =  X.loc[:,X.columns != \"Status\"]            # removing Status\n",
        "X =  X.loc[:,X.columns != \"Epoch\"]             # removing Epoch\n",
        "y = dataset[\"Activity\"]\n",
        "\n",
        "X.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "eBxa1iqT3Ocs",
        "outputId": "9a597afd-1d0b-4765-f825-f99672353cc1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   MeanP_Alpha_Fz  MedianF_Alpha_Fz  MeanF_Alpha_Fz  Spectral Edge_Alpha_Fz  \\\n",
              "0        0.000008          11.70732        28.29268                20.00000   \n",
              "1        0.000010          10.73171        28.78049                22.43902   \n",
              "2        0.000008          13.17073        28.78049                23.41463   \n",
              "3        0.000011          10.24390        27.80488                18.53659   \n",
              "4        0.000007          10.73171        23.41463                18.04878   \n",
              "5        0.000008          10.24390        23.41463                17.56098   \n",
              "6        0.000005          11.21951        24.39024                17.56098   \n",
              "7        0.000005           9.75610        25.85366                18.53659   \n",
              "8        0.000006          10.24390        24.39024                17.56098   \n",
              "9        0.000014          11.21951        23.41463                18.04878   \n",
              "\n",
              "   PeakF_Alpha_Fz  MeanP_Beta_Fz  MedianF_Beta_Fz  MeanF_Beta_Fz  \\\n",
              "0        10.73171       0.000017         19.51220       38.04878   \n",
              "1        10.24390       0.000023         22.92683       38.53659   \n",
              "2         9.26829       0.000023         22.43902       37.56098   \n",
              "3        10.73171       0.000019         22.43902       38.04878   \n",
              "4        11.70732       0.000010         17.56098       36.09756   \n",
              "5        10.24390       0.000010         18.04878       36.09756   \n",
              "6        12.68293       0.000007         17.07317       38.04878   \n",
              "7         7.80488       0.000007         19.02439       35.60976   \n",
              "8         8.29268       0.000009         17.56098       36.09756   \n",
              "9        12.68293       0.000021         18.04878       35.60976   \n",
              "\n",
              "   Spectral Edge_Beta_Fz  PeakF_Beta_Fz  ...  Relative Power_Alpha_Global  \\\n",
              "0               32.68293       19.02439  ...                     0.358154   \n",
              "1               33.65854       23.41463  ...                     0.457695   \n",
              "2               31.70732       17.56098  ...                     0.501419   \n",
              "3               32.68293       28.29268  ...                     0.497909   \n",
              "4               29.26829       11.70732  ...                     0.429825   \n",
              "5               29.26829       17.07317  ...                     0.494096   \n",
              "6               31.21951       14.14634  ...                     0.527217   \n",
              "7               29.75610       13.17073  ...                     0.475599   \n",
              "8               29.26829       17.56098  ...                     0.482293   \n",
              "9               29.26829       12.68293  ...                     0.659929   \n",
              "\n",
              "   Relative Power_Beta_Global  Relative Power_Theta_Global  \\\n",
              "0                    0.639771                     0.749628   \n",
              "1                    0.806208                     0.723662   \n",
              "2                    1.059244                     0.737332   \n",
              "3                    0.710690                     0.907816   \n",
              "4                    0.569204                     0.849345   \n",
              "5                    0.687221                     0.812251   \n",
              "6                    0.668711                     0.770164   \n",
              "7                    0.719244                     0.764508   \n",
              "8                    0.620810                     0.933737   \n",
              "9                    0.855344                     1.188749   \n",
              "\n",
              "   Relative Power_Delta_Global  Relative Power_Gamma_Global  \\\n",
              "0                     4.031903                     0.220545   \n",
              "1                     3.759249                     0.253186   \n",
              "2                     3.320235                     0.381769   \n",
              "3                     3.653423                     0.230162   \n",
              "4                     4.001597                     0.150029   \n",
              "5                     3.785048                     0.221383   \n",
              "6                     3.853441                     0.180467   \n",
              "7                     3.833454                     0.207195   \n",
              "8                     3.798965                     0.164195   \n",
              "9                     3.079654                     0.216324   \n",
              "\n",
              "   ∆Relative Power_Alpha_Global  ∆Relative Power_Beta_Global  \\\n",
              "0                     -0.273666                    -0.164639   \n",
              "1                     -0.071797                     0.052680   \n",
              "2                      0.016875                     0.383074   \n",
              "3                      0.009757                    -0.072039   \n",
              "4                     -0.128318                    -0.256780   \n",
              "5                      0.002023                    -0.102682   \n",
              "6                      0.069192                    -0.126852   \n",
              "7                     -0.035489                    -0.060870   \n",
              "8                     -0.021913                    -0.189396   \n",
              "9                      0.338331                     0.116839   \n",
              "\n",
              "   ∆Relative Power_Theta_Global  ∆Relative Power_Delta_Global  \\\n",
              "0                     -0.009611                      0.107193   \n",
              "1                     -0.043916                      0.032320   \n",
              "2                     -0.025855                     -0.088237   \n",
              "3                      0.199384                      0.003259   \n",
              "4                      0.122134                      0.098870   \n",
              "5                      0.073126                      0.039404   \n",
              "6                      0.017521                      0.058186   \n",
              "7                      0.010050                      0.052697   \n",
              "8                      0.233631                      0.043226   \n",
              "9                      0.570546                     -0.154303   \n",
              "\n",
              "   ∆Relative Power_Gamma_Global  \n",
              "0                     -0.356230  \n",
              "1                     -0.260950  \n",
              "2                      0.114385  \n",
              "3                     -0.328157  \n",
              "4                     -0.562065  \n",
              "5                     -0.353782  \n",
              "6                     -0.473217  \n",
              "7                     -0.395197  \n",
              "8                     -0.520716  \n",
              "9                     -0.368549  \n",
              "\n",
              "[10 rows x 256 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a383000-ff99-4865-82af-5897888ac601\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MeanP_Alpha_Fz</th>\n",
              "      <th>MedianF_Alpha_Fz</th>\n",
              "      <th>MeanF_Alpha_Fz</th>\n",
              "      <th>Spectral Edge_Alpha_Fz</th>\n",
              "      <th>PeakF_Alpha_Fz</th>\n",
              "      <th>MeanP_Beta_Fz</th>\n",
              "      <th>MedianF_Beta_Fz</th>\n",
              "      <th>MeanF_Beta_Fz</th>\n",
              "      <th>Spectral Edge_Beta_Fz</th>\n",
              "      <th>PeakF_Beta_Fz</th>\n",
              "      <th>...</th>\n",
              "      <th>Relative Power_Alpha_Global</th>\n",
              "      <th>Relative Power_Beta_Global</th>\n",
              "      <th>Relative Power_Theta_Global</th>\n",
              "      <th>Relative Power_Delta_Global</th>\n",
              "      <th>Relative Power_Gamma_Global</th>\n",
              "      <th>∆Relative Power_Alpha_Global</th>\n",
              "      <th>∆Relative Power_Beta_Global</th>\n",
              "      <th>∆Relative Power_Theta_Global</th>\n",
              "      <th>∆Relative Power_Delta_Global</th>\n",
              "      <th>∆Relative Power_Gamma_Global</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>11.70732</td>\n",
              "      <td>28.29268</td>\n",
              "      <td>20.00000</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>19.51220</td>\n",
              "      <td>38.04878</td>\n",
              "      <td>32.68293</td>\n",
              "      <td>19.02439</td>\n",
              "      <td>...</td>\n",
              "      <td>0.358154</td>\n",
              "      <td>0.639771</td>\n",
              "      <td>0.749628</td>\n",
              "      <td>4.031903</td>\n",
              "      <td>0.220545</td>\n",
              "      <td>-0.273666</td>\n",
              "      <td>-0.164639</td>\n",
              "      <td>-0.009611</td>\n",
              "      <td>0.107193</td>\n",
              "      <td>-0.356230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000010</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>28.78049</td>\n",
              "      <td>22.43902</td>\n",
              "      <td>10.24390</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>22.92683</td>\n",
              "      <td>38.53659</td>\n",
              "      <td>33.65854</td>\n",
              "      <td>23.41463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.457695</td>\n",
              "      <td>0.806208</td>\n",
              "      <td>0.723662</td>\n",
              "      <td>3.759249</td>\n",
              "      <td>0.253186</td>\n",
              "      <td>-0.071797</td>\n",
              "      <td>0.052680</td>\n",
              "      <td>-0.043916</td>\n",
              "      <td>0.032320</td>\n",
              "      <td>-0.260950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>13.17073</td>\n",
              "      <td>28.78049</td>\n",
              "      <td>23.41463</td>\n",
              "      <td>9.26829</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>22.43902</td>\n",
              "      <td>37.56098</td>\n",
              "      <td>31.70732</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>...</td>\n",
              "      <td>0.501419</td>\n",
              "      <td>1.059244</td>\n",
              "      <td>0.737332</td>\n",
              "      <td>3.320235</td>\n",
              "      <td>0.381769</td>\n",
              "      <td>0.016875</td>\n",
              "      <td>0.383074</td>\n",
              "      <td>-0.025855</td>\n",
              "      <td>-0.088237</td>\n",
              "      <td>0.114385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000011</td>\n",
              "      <td>10.24390</td>\n",
              "      <td>27.80488</td>\n",
              "      <td>18.53659</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>22.43902</td>\n",
              "      <td>38.04878</td>\n",
              "      <td>32.68293</td>\n",
              "      <td>28.29268</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497909</td>\n",
              "      <td>0.710690</td>\n",
              "      <td>0.907816</td>\n",
              "      <td>3.653423</td>\n",
              "      <td>0.230162</td>\n",
              "      <td>0.009757</td>\n",
              "      <td>-0.072039</td>\n",
              "      <td>0.199384</td>\n",
              "      <td>0.003259</td>\n",
              "      <td>-0.328157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000007</td>\n",
              "      <td>10.73171</td>\n",
              "      <td>23.41463</td>\n",
              "      <td>18.04878</td>\n",
              "      <td>11.70732</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>36.09756</td>\n",
              "      <td>29.26829</td>\n",
              "      <td>11.70732</td>\n",
              "      <td>...</td>\n",
              "      <td>0.429825</td>\n",
              "      <td>0.569204</td>\n",
              "      <td>0.849345</td>\n",
              "      <td>4.001597</td>\n",
              "      <td>0.150029</td>\n",
              "      <td>-0.128318</td>\n",
              "      <td>-0.256780</td>\n",
              "      <td>0.122134</td>\n",
              "      <td>0.098870</td>\n",
              "      <td>-0.562065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>10.24390</td>\n",
              "      <td>23.41463</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>10.24390</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>18.04878</td>\n",
              "      <td>36.09756</td>\n",
              "      <td>29.26829</td>\n",
              "      <td>17.07317</td>\n",
              "      <td>...</td>\n",
              "      <td>0.494096</td>\n",
              "      <td>0.687221</td>\n",
              "      <td>0.812251</td>\n",
              "      <td>3.785048</td>\n",
              "      <td>0.221383</td>\n",
              "      <td>0.002023</td>\n",
              "      <td>-0.102682</td>\n",
              "      <td>0.073126</td>\n",
              "      <td>0.039404</td>\n",
              "      <td>-0.353782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000005</td>\n",
              "      <td>11.21951</td>\n",
              "      <td>24.39024</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>12.68293</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>17.07317</td>\n",
              "      <td>38.04878</td>\n",
              "      <td>31.21951</td>\n",
              "      <td>14.14634</td>\n",
              "      <td>...</td>\n",
              "      <td>0.527217</td>\n",
              "      <td>0.668711</td>\n",
              "      <td>0.770164</td>\n",
              "      <td>3.853441</td>\n",
              "      <td>0.180467</td>\n",
              "      <td>0.069192</td>\n",
              "      <td>-0.126852</td>\n",
              "      <td>0.017521</td>\n",
              "      <td>0.058186</td>\n",
              "      <td>-0.473217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.000005</td>\n",
              "      <td>9.75610</td>\n",
              "      <td>25.85366</td>\n",
              "      <td>18.53659</td>\n",
              "      <td>7.80488</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>19.02439</td>\n",
              "      <td>35.60976</td>\n",
              "      <td>29.75610</td>\n",
              "      <td>13.17073</td>\n",
              "      <td>...</td>\n",
              "      <td>0.475599</td>\n",
              "      <td>0.719244</td>\n",
              "      <td>0.764508</td>\n",
              "      <td>3.833454</td>\n",
              "      <td>0.207195</td>\n",
              "      <td>-0.035489</td>\n",
              "      <td>-0.060870</td>\n",
              "      <td>0.010050</td>\n",
              "      <td>0.052697</td>\n",
              "      <td>-0.395197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>10.24390</td>\n",
              "      <td>24.39024</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>8.29268</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>36.09756</td>\n",
              "      <td>29.26829</td>\n",
              "      <td>17.56098</td>\n",
              "      <td>...</td>\n",
              "      <td>0.482293</td>\n",
              "      <td>0.620810</td>\n",
              "      <td>0.933737</td>\n",
              "      <td>3.798965</td>\n",
              "      <td>0.164195</td>\n",
              "      <td>-0.021913</td>\n",
              "      <td>-0.189396</td>\n",
              "      <td>0.233631</td>\n",
              "      <td>0.043226</td>\n",
              "      <td>-0.520716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000014</td>\n",
              "      <td>11.21951</td>\n",
              "      <td>23.41463</td>\n",
              "      <td>18.04878</td>\n",
              "      <td>12.68293</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>18.04878</td>\n",
              "      <td>35.60976</td>\n",
              "      <td>29.26829</td>\n",
              "      <td>12.68293</td>\n",
              "      <td>...</td>\n",
              "      <td>0.659929</td>\n",
              "      <td>0.855344</td>\n",
              "      <td>1.188749</td>\n",
              "      <td>3.079654</td>\n",
              "      <td>0.216324</td>\n",
              "      <td>0.338331</td>\n",
              "      <td>0.116839</td>\n",
              "      <td>0.570546</td>\n",
              "      <td>-0.154303</td>\n",
              "      <td>-0.368549</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 256 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a383000-ff99-4865-82af-5897888ac601')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a383000-ff99-4865-82af-5897888ac601 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a383000-ff99-4865-82af-5897888ac601');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset =  balance(X,y)"
      ],
      "metadata": {
        "id": "hHX2PZjeAPHj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Selection"
      ],
      "metadata": {
        "id": "C0Akgb_na7NU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_feat = 20"
      ],
      "metadata": {
        "id": "rY2mWiZv80L2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run only one Method"
      ],
      "metadata": {
        "id": "hBJAeCQyG9K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANOVA with f classifciation"
      ],
      "metadata": {
        "id": "cR3Hrghl7CVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "fs = SelectKBest(score_func=f_classif, k=5)\n",
        "fit = fs.fit(X,y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "\n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "\n",
        "featureScores.columns = ['Best_columns','Score_ANOVA'] \n",
        "\n",
        "lyst = featureScores.nlargest(number_of_feat,'Score_ANOVA')\n",
        "\n",
        "#lyst.to_csv('Filter_Method_ANOVA_with_f_classif.csv')\n",
        "\n",
        "list_of_feat = list(lyst[\"Best_columns\"])"
      ],
      "metadata": {
        "id": "jWdiiglz2iV-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embedded Method"
      ],
      "metadata": {
        "id": "ulApxK0w7JXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LassoCV\n",
        "reg = LassoCV()\n",
        "reg.fit(X, y)\n",
        "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
        "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\n",
        "coef = pd.Series(reg.coef_, index = X.columns)\n",
        "\n",
        "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
        "\n",
        "imp_coef = coef.sort_values()\n",
        "\n",
        "list_of_feat=[]\n",
        "\n",
        "\n",
        "for i in range(coef.shape[0]):\n",
        "  if coef[i]!=0:\n",
        "    list_of_feat.append(dataset.iloc[:0,i+3].name)\n",
        "    \n",
        "df = pd.DataFrame(list_of_feat, columns=['Best_Features'])\n",
        "\n",
        "#df.to_csv(\"Embedded_Method.csv\")\n",
        "\n",
        "list_of_feat = list(df[\"Best_Features\"])"
      ],
      "metadata": {
        "id": "2A4W1e2v5sse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c94bd7-7e32-439f-eca1-c728da10f62e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4267169530156707, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5367691825176735, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44178979448417977, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43952068898738617, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.46433912886061535, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4989548064010023, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44551548182505485, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.43447831353933, tolerance: 0.21672448830409385\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6854571622635603, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6359106571626398, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.118580555415747, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.035189381130181, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44324843964204774, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.320660533885416, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3528893170340552, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0475886722929317, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2859467656206789, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6886040710332963, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.998050559647595, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.679900168994209, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.219082027051854, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.039537952752198, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9415427907708818, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.002354145654067, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.228428024035793, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.396283604265136, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8910330314937482, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.61956106919331, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.399343404807269, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.845211129611243, tolerance: 0.19155865595325047\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6412599116704314, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.635566985978016, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.149978421512742, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0463717673933388, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1249466366007255, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9639174785226032, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.128761565914374, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.01357223245941, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9125629880059023, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.772142571907807, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8057308298861017, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.108965049155017, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.145962788478869, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8089646126340995, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5272684802470167, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9709261281667523, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.522232171109067, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.373936113512855, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.8200969666474975, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.819100593515486, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.778754823772601, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.548775913541704, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.86090235394704, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.140180840642529, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.644550679078407, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.9563886319442645, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.21672824403958657, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6587483567900563, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8016907527025978, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2918859110511676, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.200606245269455, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.712090984429551, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.7746353418871195, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.462780333756314, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.784012395572745, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.863789293087962, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.428385278902283, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.429899369219129, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.064596863876886, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.503576429383656, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.87546807781564, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6660131299462364, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.336437450820199, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2145848850275343, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0666533712883393, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.053293760534416, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.054618481903162, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.114466884816352, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.448019029321017, tolerance: 0.1750899926953981\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.11474914630593958, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19374736821760052, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.13494072588798645, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22719690201859066, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19429411930013885, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19050649603673264, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.256380345892012, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2911623253594371, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2708546981477866, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2694254374995353, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.263341127462013, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2507549959905191, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2520277898437371, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.22548068700359636, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.2049915101465558, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.29015633449625966, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.39755466511519444, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.48631634941807533, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5372703612974874, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5303664386892706, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4852303203185784, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.41299196977752217, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.44504031788073917, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6044164916149839, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5821489079249886, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.640216462648823, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9526979151553405, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.453510519992392, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.9106893127578815, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.238552191859867, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4962298619279863, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.7734824898376473, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9457448973673763, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.461659613569168, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.394426658807788, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.197293579875236, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.7140861379944, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.857260651044385, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:633: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.71257207430142, tolerance: 0.10558027757487208\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best alpha using built-in LassoCV: 0.026444\n",
            "Best score using built-in LassoCV: 0.462189\n",
            "Lasso picked 76 variables and eliminated the other 180 variables\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.166e+02, tolerance: 2.256e-01\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pearson's with f regression"
      ],
      "metadata": {
        "id": "roe4MPHM7NJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "fs = SelectKBest(score_func=f_regression, k=5)\n",
        "fit = fs.fit(X,y)\n",
        "\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(dataset.columns)\n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "\n",
        "featureScores.columns = ['Best_columns','Score_pearsons'] \n",
        "\n",
        "\n",
        "lyst = featureScores.nlargest(number_of_feat,'Score_pearsons')\n",
        "\n",
        "#lyst.to_csv('Filter_Method_Pearson’s_with_f_regression.csv')\n",
        "\n",
        "list_of_feat = list(lyst[\"Best_columns\"])"
      ],
      "metadata": {
        "id": "o4A0dH566an_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sequential Feature Selection"
      ],
      "metadata": {
        "id": "kwFK7VeE7RwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "sfs = SequentialFeatureSelector(knn, n_features_to_select=number_of_feat)\n",
        "sfs.fit(X, y)\n",
        "list_of_feat=[]\n",
        "list_of_feat=list(sfs.get_feature_names_out(X.columns))\n",
        "\n",
        "df = pd.DataFrame(list_of_feat, columns=['Best_Features'])\n",
        "\n",
        "#df.to_csv(\"Filter_Method_Sequential_feat_Selection_KNN.csv\")\n",
        "\n",
        "list_of_feat = list(df[\"Best_Features\"])"
      ],
      "metadata": {
        "id": "sQlorvfS6wed"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###All features"
      ],
      "metadata": {
        "id": "swd5ZOVO-8cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_feat = list(X.columns)"
      ],
      "metadata": {
        "id": "KoiMxNN3-_zH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Spliting"
      ],
      "metadata": {
        "id": "k4YDHMuh-wMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = new_dataset[list_of_feat]\n",
        "y_new = new_dataset[target]"
      ],
      "metadata": {
        "id": "MKgy4THrq4v8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new.shape"
      ],
      "metadata": {
        "id": "Fxz-DgDXZtpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e9e671-be88-4f38-ccd8-fdd021016058"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3172, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_new.shape"
      ],
      "metadata": {
        "id": "1q9Y_6BAr-es",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a11910-a95b-4f1f-8509-193855fdcbd6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3172,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "_CYZE4jgq3w9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bqYsoblRogp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BydV84Diooxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ADABOOST"
      ],
      "metadata": {
        "id": "jEe2R0yAoifa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_defult = AdaBoostClassifier(random_state=0)\n",
        "ada_defult.fit(X_train, y_train)\n",
        "y_pred = ada_defult.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(ada_defult,1,'AdaBoostClassifier')]=accuracy_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "Bz_yVJaXod8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1795798-2579-4c83-d2b9-83df8f24bbf1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 92  39  23   9]\n",
            " [ 42  83  14   6]\n",
            " [ 32  31  68  37]\n",
            " [  3   5  22 129]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.56      0.55       163\n",
            "           1       0.53      0.57      0.55       145\n",
            "           2       0.54      0.40      0.46       168\n",
            "           3       0.71      0.81      0.76       159\n",
            "\n",
            "    accuracy                           0.59       635\n",
            "   macro avg       0.58      0.59      0.58       635\n",
            "weighted avg       0.58      0.59      0.58       635\n",
            "\n",
            "Accurecy:  0.5858267716535434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "N=200\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = AdaBoostClassifier(n_estimators=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#plot the relationship between K and the testing accuracy\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ],
      "metadata": {
        "id": "_3wRG3rxotzH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "45418899-91ac-4b16-be4c-83182109e415"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 round completed......................... Accurecy: 0.3826771653543307\n",
            "2/200 round completed......................... Accurecy: 0.46141732283464565\n",
            "3/200 round completed......................... Accurecy: 0.4692913385826772\n",
            "4/200 round completed......................... Accurecy: 0.46771653543307085\n",
            "5/200 round completed......................... Accurecy: 0.47244094488188976\n",
            "6/200 round completed......................... Accurecy: 0.494488188976378\n",
            "7/200 round completed......................... Accurecy: 0.49291338582677163\n",
            "8/200 round completed......................... Accurecy: 0.4818897637795276\n",
            "9/200 round completed......................... Accurecy: 0.5086614173228347\n",
            "10/200 round completed......................... Accurecy: 0.5023622047244094\n",
            "11/200 round completed......................... Accurecy: 0.5070866141732283\n",
            "12/200 round completed......................... Accurecy: 0.5086614173228347\n",
            "13/200 round completed......................... Accurecy: 0.5543307086614173\n",
            "14/200 round completed......................... Accurecy: 0.552755905511811\n",
            "15/200 round completed......................... Accurecy: 0.5574803149606299\n",
            "16/200 round completed......................... Accurecy: 0.5763779527559055\n",
            "17/200 round completed......................... Accurecy: 0.573228346456693\n",
            "18/200 round completed......................... Accurecy: 0.568503937007874\n",
            "19/200 round completed......................... Accurecy: 0.5637795275590551\n",
            "20/200 round completed......................... Accurecy: 0.5622047244094488\n",
            "21/200 round completed......................... Accurecy: 0.5700787401574803\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ddcad351f807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_range\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \"\"\"\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SAMME.R\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_best_estimator = AdaBoostClassifier(n_estimators=best_estimator,random_state=0)\n",
        "ada_best_estimator.fit(X_train, y_train)\n",
        "y_pred = ada_best_estimator.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(ada_best_estimator,1,'AdaBoostClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "ctzoGCmFo0In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JQhATacCo1AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graddient Boosting"
      ],
      "metadata": {
        "id": "FJybpt_UvCU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "gradBoost_default = GradientBoostingClassifier(random_state=0)\n",
        "gradBoost_default.fit(X_train, y_train)\n",
        "y_pred = gradBoost_default.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(gradBoost_default,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "sgmvc8A_o18j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=150\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = GradientBoostingClassifier(n_estimators=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ],
      "metadata": {
        "id": "9brp-qNgo-tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=30\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = GradientBoostingClassifier(max_depth=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best Depth:\")\n",
        "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_depth)"
      ],
      "metadata": {
        "id": "JtO5QAkHpDAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradBoost_estimator = GradientBoostingClassifier(n_estimators=best_estimator,random_state=0)\n",
        "gradBoost_estimator.fit(X_train, y_train)\n",
        "y_pred = gradBoost_estimator.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(gradBoost_estimator,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "EFg4mMYepGXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradBoost_depth = GradientBoostingClassifier(max_depth=best_depth,random_state=0)\n",
        "gradBoost_depth.fit(X_train, y_train)\n",
        "y_pred = gradBoost_depth.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "\n",
        "result[(gradBoost_depth,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "oS_ycrCQpI09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradBoost_all = GradientBoostingClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
        "gradBoost_all.fit(X_train, y_train)\n",
        "y_pred = gradBoost_all.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "\n",
        "result[(gradBoost_all,2,'GradientBoostingClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "GU8wNje3pJjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ThoTlhc4pRJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "AgMT_U8gvM7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_default = RandomForestClassifier(random_state=0)\n",
        "rf_default.fit(X_train, y_train)\n",
        "y_pred=rf_default.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_default,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "-4GMunEvtWk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=150\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = RandomForestClassifier(n_estimators=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ],
      "metadata": {
        "id": "ve34YgestieE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=150\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  classifier = RandomForestClassifier(max_depth=k,random_state=0)\n",
        "  classifier.fit(X_train, y_train)\n",
        "  y_pred=classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best Depth:\")\n",
        "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_depth)"
      ],
      "metadata": {
        "id": "eVfKbSTWtnny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_estimator = RandomForestClassifier(n_estimators=best_estimator,random_state=0)\n",
        "rf_estimator.fit(X_train, y_train)\n",
        "y_pred=rf_estimator.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_estimator,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "LMUwwEqUt2-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_depth = RandomForestClassifier(max_depth=best_depth,random_state=0)\n",
        "rf_depth.fit(X_train, y_train)\n",
        "y_pred=rf_depth.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_depth,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "NRybS4-nt_vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_all = RandomForestClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
        "rf_all.fit(X_train, y_train)\n",
        "y_pred=rf_all.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(rf_all,3,'RandomForestClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "G9p0fJyBtrdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G_DChNuquB92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGB"
      ],
      "metadata": {
        "id": "jRG4cBnKvZU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "xgb_deafult = xgb.XGBClassifier(random_state=0)\n",
        "xgb_deafult.fit(X_train,y_train)\n",
        "y_pred = xgb_deafult.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_deafult,4,'xgboost')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "sC1lAZeeuCw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd32c49b-7903-48c7-df60-49db4fb18bb2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[129  16  17   1]\n",
            " [ 13 126   3   3]\n",
            " [ 14  10 119  25]\n",
            " [  3   3  19 134]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.79      0.80       163\n",
            "           1       0.81      0.87      0.84       145\n",
            "           2       0.75      0.71      0.73       168\n",
            "           3       0.82      0.84      0.83       159\n",
            "\n",
            "    accuracy                           0.80       635\n",
            "   macro avg       0.80      0.80      0.80       635\n",
            "weighted avg       0.80      0.80      0.80       635\n",
            "\n",
            "Accurecy:  0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=250\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  xgb_classifier = xgb.XGBClassifier(n_estimators=k,random_state=0)\n",
        "  xgb_classifier.fit(X_train, y_train)\n",
        "  y_pred=xgb_classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best n_estimators:\")\n",
        "best_estimator=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_estimator)"
      ],
      "metadata": {
        "id": "YQYsfVO0uPMA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "68fac225-4501-4ded-8f6b-53276b6959de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/250 round completed......................... Accurecy: 0.6236220472440945\n",
            "2/250 round completed......................... Accurecy: 0.6346456692913386\n",
            "3/250 round completed......................... Accurecy: 0.6440944881889764\n",
            "4/250 round completed......................... Accurecy: 0.6519685039370079\n",
            "5/250 round completed......................... Accurecy: 0.647244094488189\n",
            "6/250 round completed......................... Accurecy: 0.662992125984252\n",
            "7/250 round completed......................... Accurecy: 0.6598425196850394\n",
            "8/250 round completed......................... Accurecy: 0.6787401574803149\n",
            "9/250 round completed......................... Accurecy: 0.6803149606299213\n",
            "10/250 round completed......................... Accurecy: 0.6740157480314961\n",
            "11/250 round completed......................... Accurecy: 0.6834645669291338\n",
            "12/250 round completed......................... Accurecy: 0.6834645669291338\n",
            "13/250 round completed......................... Accurecy: 0.6881889763779527\n",
            "14/250 round completed......................... Accurecy: 0.694488188976378\n",
            "15/250 round completed......................... Accurecy: 0.6913385826771653\n",
            "16/250 round completed......................... Accurecy: 0.6929133858267716\n",
            "17/250 round completed......................... Accurecy: 0.7023622047244095\n",
            "18/250 round completed......................... Accurecy: 0.7007874015748031\n",
            "19/250 round completed......................... Accurecy: 0.7070866141732284\n",
            "20/250 round completed......................... Accurecy: 0.7118110236220473\n",
            "21/250 round completed......................... Accurecy: 0.710236220472441\n",
            "22/250 round completed......................... Accurecy: 0.7086614173228346\n",
            "23/250 round completed......................... Accurecy: 0.7165354330708661\n",
            "24/250 round completed......................... Accurecy: 0.7181102362204724\n",
            "25/250 round completed......................... Accurecy: 0.7181102362204724\n",
            "26/250 round completed......................... Accurecy: 0.7196850393700788\n",
            "27/250 round completed......................... Accurecy: 0.721259842519685\n",
            "28/250 round completed......................... Accurecy: 0.7275590551181103\n",
            "29/250 round completed......................... Accurecy: 0.7259842519685039\n",
            "30/250 round completed......................... Accurecy: 0.7275590551181103\n",
            "31/250 round completed......................... Accurecy: 0.7338582677165354\n",
            "32/250 round completed......................... Accurecy: 0.7338582677165354\n",
            "33/250 round completed......................... Accurecy: 0.7338582677165354\n",
            "34/250 round completed......................... Accurecy: 0.7354330708661417\n",
            "35/250 round completed......................... Accurecy: 0.7354330708661417\n",
            "36/250 round completed......................... Accurecy: 0.7354330708661417\n",
            "37/250 round completed......................... Accurecy: 0.7354330708661417\n",
            "38/250 round completed......................... Accurecy: 0.7401574803149606\n",
            "39/250 round completed......................... Accurecy: 0.7401574803149606\n",
            "40/250 round completed......................... Accurecy: 0.7385826771653543\n",
            "41/250 round completed......................... Accurecy: 0.7417322834645669\n",
            "42/250 round completed......................... Accurecy: 0.7370078740157481\n",
            "43/250 round completed......................... Accurecy: 0.7385826771653543\n",
            "44/250 round completed......................... Accurecy: 0.7417322834645669\n",
            "45/250 round completed......................... Accurecy: 0.7464566929133858\n",
            "46/250 round completed......................... Accurecy: 0.7511811023622047\n",
            "47/250 round completed......................... Accurecy: 0.7464566929133858\n",
            "48/250 round completed......................... Accurecy: 0.752755905511811\n",
            "49/250 round completed......................... Accurecy: 0.7559055118110236\n",
            "50/250 round completed......................... Accurecy: 0.7590551181102362\n",
            "51/250 round completed......................... Accurecy: 0.75748031496063\n",
            "52/250 round completed......................... Accurecy: 0.752755905511811\n",
            "53/250 round completed......................... Accurecy: 0.7543307086614173\n",
            "54/250 round completed......................... Accurecy: 0.7590551181102362\n",
            "55/250 round completed......................... Accurecy: 0.7606299212598425\n",
            "56/250 round completed......................... Accurecy: 0.7590551181102362\n",
            "57/250 round completed......................... Accurecy: 0.7590551181102362\n",
            "58/250 round completed......................... Accurecy: 0.7622047244094489\n",
            "59/250 round completed......................... Accurecy: 0.7669291338582677\n",
            "60/250 round completed......................... Accurecy: 0.7700787401574803\n",
            "61/250 round completed......................... Accurecy: 0.7700787401574803\n",
            "62/250 round completed......................... Accurecy: 0.7700787401574803\n",
            "63/250 round completed......................... Accurecy: 0.7716535433070866\n",
            "64/250 round completed......................... Accurecy: 0.7732283464566929\n",
            "65/250 round completed......................... Accurecy: 0.7748031496062993\n",
            "66/250 round completed......................... Accurecy: 0.7826771653543307\n",
            "67/250 round completed......................... Accurecy: 0.7826771653543307\n",
            "68/250 round completed......................... Accurecy: 0.7795275590551181\n",
            "69/250 round completed......................... Accurecy: 0.7811023622047244\n",
            "70/250 round completed......................... Accurecy: 0.784251968503937\n",
            "71/250 round completed......................... Accurecy: 0.7811023622047244\n",
            "72/250 round completed......................... Accurecy: 0.7811023622047244\n",
            "73/250 round completed......................... Accurecy: 0.7811023622047244\n",
            "74/250 round completed......................... Accurecy: 0.7826771653543307\n",
            "75/250 round completed......................... Accurecy: 0.784251968503937\n",
            "76/250 round completed......................... Accurecy: 0.7858267716535433\n",
            "77/250 round completed......................... Accurecy: 0.7889763779527559\n",
            "78/250 round completed......................... Accurecy: 0.7889763779527559\n",
            "79/250 round completed......................... Accurecy: 0.7889763779527559\n",
            "80/250 round completed......................... Accurecy: 0.7874015748031497\n",
            "81/250 round completed......................... Accurecy: 0.7905511811023622\n",
            "82/250 round completed......................... Accurecy: 0.7889763779527559\n",
            "83/250 round completed......................... Accurecy: 0.7874015748031497\n",
            "84/250 round completed......................... Accurecy: 0.7874015748031497\n",
            "85/250 round completed......................... Accurecy: 0.7889763779527559\n",
            "86/250 round completed......................... Accurecy: 0.7905511811023622\n",
            "87/250 round completed......................... Accurecy: 0.7968503937007874\n",
            "88/250 round completed......................... Accurecy: 0.7968503937007874\n",
            "89/250 round completed......................... Accurecy: 0.7937007874015748\n",
            "90/250 round completed......................... Accurecy: 0.7952755905511811\n",
            "91/250 round completed......................... Accurecy: 0.7952755905511811\n",
            "92/250 round completed......................... Accurecy: 0.7921259842519685\n",
            "93/250 round completed......................... Accurecy: 0.7968503937007874\n",
            "94/250 round completed......................... Accurecy: 0.7968503937007874\n",
            "95/250 round completed......................... Accurecy: 0.7968503937007874\n",
            "96/250 round completed......................... Accurecy: 0.7984251968503937\n",
            "97/250 round completed......................... Accurecy: 0.8\n",
            "98/250 round completed......................... Accurecy: 0.8\n",
            "99/250 round completed......................... Accurecy: 0.8\n",
            "100/250 round completed......................... Accurecy: 0.8\n",
            "101/250 round completed......................... Accurecy: 0.7984251968503937\n",
            "102/250 round completed......................... Accurecy: 0.8\n",
            "103/250 round completed......................... Accurecy: 0.8\n",
            "104/250 round completed......................... Accurecy: 0.8015748031496063\n",
            "105/250 round completed......................... Accurecy: 0.8031496062992126\n",
            "106/250 round completed......................... Accurecy: 0.8\n",
            "107/250 round completed......................... Accurecy: 0.8015748031496063\n",
            "108/250 round completed......................... Accurecy: 0.8031496062992126\n",
            "109/250 round completed......................... Accurecy: 0.8047244094488188\n",
            "110/250 round completed......................... Accurecy: 0.8047244094488188\n",
            "111/250 round completed......................... Accurecy: 0.8031496062992126\n",
            "112/250 round completed......................... Accurecy: 0.8078740157480315\n",
            "113/250 round completed......................... Accurecy: 0.8094488188976378\n",
            "114/250 round completed......................... Accurecy: 0.8110236220472441\n",
            "115/250 round completed......................... Accurecy: 0.8094488188976378\n",
            "116/250 round completed......................... Accurecy: 0.8094488188976378\n",
            "117/250 round completed......................... Accurecy: 0.8094488188976378\n",
            "118/250 round completed......................... Accurecy: 0.8110236220472441\n",
            "119/250 round completed......................... Accurecy: 0.8094488188976378\n",
            "120/250 round completed......................... Accurecy: 0.8141732283464567\n",
            "121/250 round completed......................... Accurecy: 0.815748031496063\n",
            "122/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "123/250 round completed......................... Accurecy: 0.815748031496063\n",
            "124/250 round completed......................... Accurecy: 0.815748031496063\n",
            "125/250 round completed......................... Accurecy: 0.8173228346456692\n",
            "126/250 round completed......................... Accurecy: 0.815748031496063\n",
            "127/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "128/250 round completed......................... Accurecy: 0.8173228346456692\n",
            "129/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "130/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "131/250 round completed......................... Accurecy: 0.8220472440944881\n",
            "132/250 round completed......................... Accurecy: 0.8220472440944881\n",
            "133/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "134/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "135/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "136/250 round completed......................... Accurecy: 0.8220472440944881\n",
            "137/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "138/250 round completed......................... Accurecy: 0.8173228346456692\n",
            "139/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "140/250 round completed......................... Accurecy: 0.815748031496063\n",
            "141/250 round completed......................... Accurecy: 0.815748031496063\n",
            "142/250 round completed......................... Accurecy: 0.8173228346456692\n",
            "143/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "144/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "145/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "146/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "147/250 round completed......................... Accurecy: 0.8220472440944881\n",
            "148/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "149/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "150/250 round completed......................... Accurecy: 0.8220472440944881\n",
            "151/250 round completed......................... Accurecy: 0.8251968503937008\n",
            "152/250 round completed......................... Accurecy: 0.8251968503937008\n",
            "153/250 round completed......................... Accurecy: 0.8267716535433071\n",
            "154/250 round completed......................... Accurecy: 0.8251968503937008\n",
            "155/250 round completed......................... Accurecy: 0.8236220472440945\n",
            "156/250 round completed......................... Accurecy: 0.8236220472440945\n",
            "157/250 round completed......................... Accurecy: 0.8204724409448819\n",
            "158/250 round completed......................... Accurecy: 0.8188976377952756\n",
            "159/250 round completed......................... Accurecy: 0.8251968503937008\n",
            "160/250 round completed......................... Accurecy: 0.8267716535433071\n",
            "161/250 round completed......................... Accurecy: 0.8251968503937008\n",
            "162/250 round completed......................... Accurecy: 0.8267716535433071\n",
            "163/250 round completed......................... Accurecy: 0.8283464566929134\n",
            "164/250 round completed......................... Accurecy: 0.8299212598425196\n",
            "165/250 round completed......................... Accurecy: 0.8283464566929134\n",
            "166/250 round completed......................... Accurecy: 0.8236220472440945\n",
            "167/250 round completed......................... Accurecy: 0.8251968503937008\n",
            "168/250 round completed......................... Accurecy: 0.8267716535433071\n",
            "169/250 round completed......................... Accurecy: 0.8267716535433071\n",
            "170/250 round completed......................... Accurecy: 0.8330708661417323\n",
            "171/250 round completed......................... Accurecy: 0.831496062992126\n",
            "172/250 round completed......................... Accurecy: 0.8330708661417323\n",
            "173/250 round completed......................... Accurecy: 0.8330708661417323\n",
            "174/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "175/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "176/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "177/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "178/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "179/250 round completed......................... Accurecy: 0.8330708661417323\n",
            "180/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "181/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "182/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "183/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "184/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "185/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "186/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "187/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "188/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "189/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "190/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "191/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "192/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "193/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "194/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "195/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "196/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "197/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "198/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "199/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "200/250 round completed......................... Accurecy: 0.8330708661417323\n",
            "201/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "202/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "203/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "204/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "205/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "206/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "207/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "208/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "209/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "210/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "211/250 round completed......................... Accurecy: 0.8346456692913385\n",
            "212/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "213/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "214/250 round completed......................... Accurecy: 0.8409448818897638\n",
            "215/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "216/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "217/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "218/250 round completed......................... Accurecy: 0.8409448818897638\n",
            "219/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "220/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "221/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "222/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "223/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "224/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "225/250 round completed......................... Accurecy: 0.8409448818897638\n",
            "226/250 round completed......................... Accurecy: 0.8409448818897638\n",
            "227/250 round completed......................... Accurecy: 0.8409448818897638\n",
            "228/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "229/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "230/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "231/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "232/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "233/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "234/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "235/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "236/250 round completed......................... Accurecy: 0.8409448818897638\n",
            "237/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "238/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "239/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "240/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "241/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "242/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "243/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "244/250 round completed......................... Accurecy: 0.8362204724409449\n",
            "245/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "246/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "247/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "248/250 round completed......................... Accurecy: 0.8377952755905512\n",
            "249/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "250/250 round completed......................... Accurecy: 0.8393700787401575\n",
            "The best n_estimators:\n",
            "214\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "N=250\n",
        "k_range = range (1,N+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  xgb_classifier = xgb.XGBClassifier(max_depth=k,random_state=0)\n",
        "  xgb_classifier.fit(X_train, y_train)\n",
        "  y_pred=xgb_classifier.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(N)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best depth:\")\n",
        "best_depth=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best_depth)"
      ],
      "metadata": {
        "id": "ABex6e-3kxuX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "71d26b69-1b32-48a4-8e0f-1e3f664c2dfd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/250 round completed......................... Accurecy: 0.6645669291338583\n",
            "2/250 round completed......................... Accurecy: 0.7354330708661417\n",
            "3/250 round completed......................... Accurecy: 0.8\n",
            "4/250 round completed......................... Accurecy: 0.8409448818897638\n",
            "5/250 round completed......................... Accurecy: 0.8535433070866142\n",
            "6/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "7/250 round completed......................... Accurecy: 0.8818897637795275\n",
            "8/250 round completed......................... Accurecy: 0.8803149606299212\n",
            "9/250 round completed......................... Accurecy: 0.8755905511811024\n",
            "10/250 round completed......................... Accurecy: 0.8803149606299212\n",
            "11/250 round completed......................... Accurecy: 0.8866141732283465\n",
            "12/250 round completed......................... Accurecy: 0.8850393700787401\n",
            "13/250 round completed......................... Accurecy: 0.8740157480314961\n",
            "14/250 round completed......................... Accurecy: 0.8677165354330708\n",
            "15/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "16/250 round completed......................... Accurecy: 0.8740157480314961\n",
            "17/250 round completed......................... Accurecy: 0.8740157480314961\n",
            "18/250 round completed......................... Accurecy: 0.8755905511811024\n",
            "19/250 round completed......................... Accurecy: 0.8708661417322835\n",
            "20/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "21/250 round completed......................... Accurecy: 0.8724409448818897\n",
            "22/250 round completed......................... Accurecy: 0.8724409448818897\n",
            "23/250 round completed......................... Accurecy: 0.8740157480314961\n",
            "24/250 round completed......................... Accurecy: 0.8708661417322835\n",
            "25/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "26/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "27/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "28/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "29/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "30/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "31/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "32/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "33/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "34/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "35/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "36/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "37/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "38/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "39/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "40/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "41/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "42/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "43/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "44/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "45/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "46/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "47/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "48/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "49/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "50/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "51/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "52/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "53/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "54/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "55/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "56/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "57/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "58/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "59/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "60/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "61/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "62/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "63/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "64/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "65/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "66/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "67/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "68/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "69/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "70/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "71/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "72/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "73/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "74/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "75/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "76/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "77/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "78/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "79/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "80/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "81/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "82/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "83/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "84/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "85/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "86/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "87/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "88/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "89/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "90/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "91/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "92/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "93/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "94/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "95/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "96/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "97/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "98/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "99/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "100/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "101/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "102/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "103/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "104/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "105/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "106/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "107/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "108/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "109/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "110/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "111/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "112/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "113/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "114/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "115/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "116/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "117/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "118/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "119/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "120/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "121/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "122/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "123/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "124/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "125/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "126/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "127/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "128/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "129/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "130/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "131/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "132/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "133/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "134/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "135/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "136/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "137/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "138/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "139/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "140/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "141/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "142/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "143/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "144/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "145/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "146/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "147/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "148/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "149/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "150/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "151/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "152/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "153/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "154/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "155/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "156/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "157/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "158/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "159/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "160/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "161/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "162/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "163/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "164/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "165/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "166/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "167/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "168/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "169/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "170/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "171/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "172/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "173/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "174/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "175/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "176/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "177/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "178/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "179/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "180/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "181/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "182/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "183/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "184/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "185/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "186/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "187/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "188/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "189/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "190/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "191/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "192/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "193/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "194/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "195/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "196/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "197/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "198/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "199/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "200/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "201/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "202/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "203/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "204/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "205/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "206/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "207/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "208/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "209/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "210/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "211/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "212/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "213/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "214/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "215/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "216/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "217/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "218/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "219/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "220/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "221/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "222/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "223/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "224/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "225/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "226/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "227/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "228/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "229/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "230/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "231/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "232/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "233/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "234/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "235/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "236/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "237/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "238/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "239/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "240/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "241/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "242/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "243/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "244/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "245/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "246/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "247/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "248/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "249/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "250/250 round completed......................... Accurecy: 0.8692913385826772\n",
            "The best depth:\n",
            "11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb_depth = xgb.XGBClassifier(max_depth=best_depth,random_state=0)\n",
        "xgb_depth.fit(X_train,y_train)\n",
        "y_pred = xgb_depth.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_depth,4,'xgboost')]=accuracy_score(y_test, y_pred)\n",
        "print(xgb_depth)"
      ],
      "metadata": {
        "id": "lfBp-uGXuaKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978d5eb4-2829-48bc-b24f-16587304ede5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[138   9  16   0]\n",
            " [  6 136   2   1]\n",
            " [ 11   6 137  14]\n",
            " [  3   1   3 152]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86       163\n",
            "           1       0.89      0.94      0.92       145\n",
            "           2       0.87      0.82      0.84       168\n",
            "           3       0.91      0.96      0.93       159\n",
            "\n",
            "    accuracy                           0.89       635\n",
            "   macro avg       0.89      0.89      0.89       635\n",
            "weighted avg       0.89      0.89      0.89       635\n",
            "\n",
            "Accurecy:  0.8866141732283465\n",
            "XGBClassifier(max_depth=11, objective='multi:softprob')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb_estimator = xgb.XGBClassifier(n_estimators=best_estimator,random_state=0)\n",
        "xgb_estimator.fit(X_train,y_train)\n",
        "y_pred = xgb_estimator.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_estimator,4,'xgboost')]=accuracy_score(y_test, y_pred)\n",
        "print(xgb_estimator)"
      ],
      "metadata": {
        "id": "_qBm-yTqzOsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539dd32b-e11c-40d8-d9d9-f6cb78bd2f3f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[131  15  16   1]\n",
            " [  7 135   3   0]\n",
            " [ 14  10 126  18]\n",
            " [  4   2  11 142]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.80      0.82       163\n",
            "           1       0.83      0.93      0.88       145\n",
            "           2       0.81      0.75      0.78       168\n",
            "           3       0.88      0.89      0.89       159\n",
            "\n",
            "    accuracy                           0.84       635\n",
            "   macro avg       0.84      0.84      0.84       635\n",
            "weighted avg       0.84      0.84      0.84       635\n",
            "\n",
            "Accurecy:  0.8409448818897638\n",
            "XGBClassifier(n_estimators=214, objective='multi:softprob')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb_all = xgb.XGBClassifier(n_estimators=best_estimator,max_depth=best_depth,random_state=0)\n",
        "xgb_all.fit(X_train,y_train)\n",
        "y_pred = xgb_all.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(xgb_all,4,'xgboost')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "zW-0PfT-zX3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e6c861-56de-4442-dfdc-079e4605efe1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[135  11  17   0]\n",
            " [  7 137   1   0]\n",
            " [ 12   6 138  12]\n",
            " [  3   1   3 152]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       163\n",
            "           1       0.88      0.94      0.91       145\n",
            "           2       0.87      0.82      0.84       168\n",
            "           3       0.93      0.96      0.94       159\n",
            "\n",
            "    accuracy                           0.89       635\n",
            "   macro avg       0.88      0.89      0.89       635\n",
            "weighted avg       0.88      0.89      0.88       635\n",
            "\n",
            "Accurecy:  0.8850393700787401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KNN"
      ],
      "metadata": {
        "id": "K6_icFWdauSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_default = KNeighborsClassifier()\n",
        "knn_default.fit(X_train, y_train)\n",
        "y_pred=knn_default.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(knn_default,5,'KNeighborsClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "6GALUBYbaz1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf835e5-4f27-4058-e512-3f8022043c2b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[104  25  27   7]\n",
            " [  9 134   2   0]\n",
            " [ 12  13 126  17]\n",
            " [  5   1   9 144]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.64      0.71       163\n",
            "           1       0.77      0.92      0.84       145\n",
            "           2       0.77      0.75      0.76       168\n",
            "           3       0.86      0.91      0.88       159\n",
            "\n",
            "    accuracy                           0.80       635\n",
            "   macro avg       0.80      0.80      0.80       635\n",
            "weighted avg       0.80      0.80      0.80       635\n",
            "\n",
            "Accurecy:  0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "Neighbors=105\n",
        "k_range = range (1,Neighbors+1)\n",
        "scores={}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(X_train, y_train)\n",
        "  y_pred=knn.predict(X_test)\n",
        "  scores[k] = accuracy_score(y_test,y_pred)\n",
        "  scores_list.append(accuracy_score(y_test,y_pred))\n",
        "  print(str(k)+\"/\"+str(Neighbors)+\" round completed......................... Accurecy: \"+str(accuracy_score(y_test,y_pred)))\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "plt.plot(k_range,scores_list)\n",
        "plt.xlabel('Value of n_estimators')\n",
        "plt.ylabel ('Testing Accuracy')\n",
        "\n",
        "\n",
        "\n",
        "print(\"The best Depth:\")\n",
        "best=list(scores.keys())[scores_list.index(max(scores_list))]\n",
        "print(best)"
      ],
      "metadata": {
        "id": "CpSsmYDHa-VF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6d2e7504-00be-447e-a0e6-d1c47f3dea47"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/105 round completed......................... Accurecy: 0.8850393700787401\n",
            "2/105 round completed......................... Accurecy: 0.8519685039370078\n",
            "3/105 round completed......................... Accurecy: 0.831496062992126\n",
            "4/105 round completed......................... Accurecy: 0.8236220472440945\n",
            "5/105 round completed......................... Accurecy: 0.8\n",
            "6/105 round completed......................... Accurecy: 0.7952755905511811\n",
            "7/105 round completed......................... Accurecy: 0.7905511811023622\n",
            "8/105 round completed......................... Accurecy: 0.7779527559055118\n",
            "9/105 round completed......................... Accurecy: 0.768503937007874\n",
            "10/105 round completed......................... Accurecy: 0.7622047244094489\n",
            "11/105 round completed......................... Accurecy: 0.7559055118110236\n",
            "12/105 round completed......................... Accurecy: 0.7543307086614173\n",
            "13/105 round completed......................... Accurecy: 0.7291338582677165\n",
            "14/105 round completed......................... Accurecy: 0.7228346456692913\n",
            "15/105 round completed......................... Accurecy: 0.7118110236220473\n",
            "16/105 round completed......................... Accurecy: 0.7070866141732284\n",
            "17/105 round completed......................... Accurecy: 0.6929133858267716\n",
            "18/105 round completed......................... Accurecy: 0.6992125984251969\n",
            "19/105 round completed......................... Accurecy: 0.7023622047244095\n",
            "20/105 round completed......................... Accurecy: 0.6960629921259842\n",
            "21/105 round completed......................... Accurecy: 0.6881889763779527\n",
            "22/105 round completed......................... Accurecy: 0.6818897637795276\n",
            "23/105 round completed......................... Accurecy: 0.6897637795275591\n",
            "24/105 round completed......................... Accurecy: 0.6913385826771653\n",
            "25/105 round completed......................... Accurecy: 0.7007874015748031\n",
            "26/105 round completed......................... Accurecy: 0.6976377952755906\n",
            "27/105 round completed......................... Accurecy: 0.6913385826771653\n",
            "28/105 round completed......................... Accurecy: 0.6881889763779527\n",
            "29/105 round completed......................... Accurecy: 0.6929133858267716\n",
            "30/105 round completed......................... Accurecy: 0.6881889763779527\n",
            "31/105 round completed......................... Accurecy: 0.6881889763779527\n",
            "32/105 round completed......................... Accurecy: 0.6771653543307087\n",
            "33/105 round completed......................... Accurecy: 0.6803149606299213\n",
            "34/105 round completed......................... Accurecy: 0.6708661417322834\n",
            "35/105 round completed......................... Accurecy: 0.6771653543307087\n",
            "36/105 round completed......................... Accurecy: 0.6661417322834645\n",
            "37/105 round completed......................... Accurecy: 0.6708661417322834\n",
            "38/105 round completed......................... Accurecy: 0.658267716535433\n",
            "39/105 round completed......................... Accurecy: 0.6598425196850394\n",
            "40/105 round completed......................... Accurecy: 0.6661417322834645\n",
            "41/105 round completed......................... Accurecy: 0.6614173228346457\n",
            "42/105 round completed......................... Accurecy: 0.662992125984252\n",
            "43/105 round completed......................... Accurecy: 0.6519685039370079\n",
            "44/105 round completed......................... Accurecy: 0.6551181102362205\n",
            "45/105 round completed......................... Accurecy: 0.6440944881889764\n",
            "46/105 round completed......................... Accurecy: 0.647244094488189\n",
            "47/105 round completed......................... Accurecy: 0.6456692913385826\n",
            "48/105 round completed......................... Accurecy: 0.6393700787401575\n",
            "49/105 round completed......................... Accurecy: 0.6362204724409449\n",
            "50/105 round completed......................... Accurecy: 0.6251968503937008\n",
            "51/105 round completed......................... Accurecy: 0.6236220472440945\n",
            "52/105 round completed......................... Accurecy: 0.6220472440944882\n",
            "53/105 round completed......................... Accurecy: 0.6204724409448819\n",
            "54/105 round completed......................... Accurecy: 0.6141732283464567\n",
            "55/105 round completed......................... Accurecy: 0.6141732283464567\n",
            "56/105 round completed......................... Accurecy: 0.6110236220472441\n",
            "57/105 round completed......................... Accurecy: 0.6110236220472441\n",
            "58/105 round completed......................... Accurecy: 0.6062992125984252\n",
            "59/105 round completed......................... Accurecy: 0.6015748031496063\n",
            "60/105 round completed......................... Accurecy: 0.6015748031496063\n",
            "61/105 round completed......................... Accurecy: 0.6\n",
            "62/105 round completed......................... Accurecy: 0.6015748031496063\n",
            "63/105 round completed......................... Accurecy: 0.5952755905511811\n",
            "64/105 round completed......................... Accurecy: 0.6078740157480315\n",
            "65/105 round completed......................... Accurecy: 0.6015748031496063\n",
            "66/105 round completed......................... Accurecy: 0.6062992125984252\n",
            "67/105 round completed......................... Accurecy: 0.5952755905511811\n",
            "68/105 round completed......................... Accurecy: 0.5984251968503937\n",
            "69/105 round completed......................... Accurecy: 0.5905511811023622\n",
            "70/105 round completed......................... Accurecy: 0.5937007874015748\n",
            "71/105 round completed......................... Accurecy: 0.5921259842519685\n",
            "72/105 round completed......................... Accurecy: 0.5889763779527559\n",
            "73/105 round completed......................... Accurecy: 0.5874015748031496\n",
            "74/105 round completed......................... Accurecy: 0.5874015748031496\n",
            "75/105 round completed......................... Accurecy: 0.5826771653543307\n",
            "76/105 round completed......................... Accurecy: 0.5826771653543307\n",
            "77/105 round completed......................... Accurecy: 0.5779527559055118\n",
            "78/105 round completed......................... Accurecy: 0.5795275590551181\n",
            "79/105 round completed......................... Accurecy: 0.5795275590551181\n",
            "80/105 round completed......................... Accurecy: 0.5795275590551181\n",
            "81/105 round completed......................... Accurecy: 0.5795275590551181\n",
            "82/105 round completed......................... Accurecy: 0.5811023622047244\n",
            "83/105 round completed......................... Accurecy: 0.5795275590551181\n",
            "84/105 round completed......................... Accurecy: 0.5763779527559055\n",
            "85/105 round completed......................... Accurecy: 0.5748031496062992\n",
            "86/105 round completed......................... Accurecy: 0.5748031496062992\n",
            "87/105 round completed......................... Accurecy: 0.5811023622047244\n",
            "88/105 round completed......................... Accurecy: 0.5763779527559055\n",
            "89/105 round completed......................... Accurecy: 0.5779527559055118\n",
            "90/105 round completed......................... Accurecy: 0.5779527559055118\n",
            "91/105 round completed......................... Accurecy: 0.5779527559055118\n",
            "92/105 round completed......................... Accurecy: 0.5748031496062992\n",
            "93/105 round completed......................... Accurecy: 0.5700787401574803\n",
            "94/105 round completed......................... Accurecy: 0.5637795275590551\n",
            "95/105 round completed......................... Accurecy: 0.5700787401574803\n",
            "96/105 round completed......................... Accurecy: 0.5637795275590551\n",
            "97/105 round completed......................... Accurecy: 0.568503937007874\n",
            "98/105 round completed......................... Accurecy: 0.568503937007874\n",
            "99/105 round completed......................... Accurecy: 0.568503937007874\n",
            "100/105 round completed......................... Accurecy: 0.5653543307086614\n",
            "101/105 round completed......................... Accurecy: 0.5622047244094488\n",
            "102/105 round completed......................... Accurecy: 0.5574803149606299\n",
            "103/105 round completed......................... Accurecy: 0.5574803149606299\n",
            "104/105 round completed......................... Accurecy: 0.5559055118110237\n",
            "105/105 round completed......................... Accurecy: 0.5574803149606299\n",
            "The best Depth:\n",
            "1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_neighbors = KNeighborsClassifier(n_neighbors=best)\n",
        "knn_neighbors.fit(X_train, y_train)\n",
        "y_pred=knn_neighbors.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(knn_neighbors,5,'KNeighborsClassifier')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "MIVJZYV8bCjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d001b4-baf3-4e62-84aa-f05512c4771e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[115  21  20   7]\n",
            " [  2 143   0   0]\n",
            " [  5   5 151   7]\n",
            " [  1   0   5 153]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.71      0.80       163\n",
            "           1       0.85      0.99      0.91       145\n",
            "           2       0.86      0.90      0.88       168\n",
            "           3       0.92      0.96      0.94       159\n",
            "\n",
            "    accuracy                           0.89       635\n",
            "   macro avg       0.89      0.89      0.88       635\n",
            "weighted avg       0.89      0.89      0.88       635\n",
            "\n",
            "Accurecy:  0.8850393700787401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NB"
      ],
      "metadata": {
        "id": "_Btki9jRvc1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_deafult = GaussianNB()\n",
        "nb_deafult.fit(X_train, y_train)\n",
        "y_pred = nb_deafult.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(\"Accurecy: \",accuracy_score(y_test, y_pred))\n",
        "result[(nb_deafult,6,'GaussianNB')]=accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "iRmd7ve-ubcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41bfc1e-ad22-43ee-c190-fe17b903e1d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[140   5   8  10]\n",
            " [118  12   1  14]\n",
            " [ 86   5  47  30]\n",
            " [ 29   3  10 117]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.86      0.52       163\n",
            "           1       0.48      0.08      0.14       145\n",
            "           2       0.71      0.28      0.40       168\n",
            "           3       0.68      0.74      0.71       159\n",
            "\n",
            "    accuracy                           0.50       635\n",
            "   macro avg       0.56      0.49      0.44       635\n",
            "weighted avg       0.57      0.50      0.45       635\n",
            "\n",
            "Accurecy:  0.49763779527559054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qqsWPAB3uv72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Result"
      ],
      "metadata": {
        "id": "6FEsxCdvkg2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models=[]\n",
        "\n",
        "for i in result:\n",
        "  models.append(i[0])\n",
        "  print(i[0],i[1],\" : \",result[i])\n",
        "  print(\"---------------------------------------------------------------\")\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "rx7qCLU14Aim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49dd5c2-11fd-4179-9718-5a2012e127e2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoostClassifier(random_state=0) 1  :  0.5858267716535434\n",
            "---------------------------------------------------------------\n",
            "\n",
            "XGBClassifier(objective='multi:softprob') 4  :  0.8\n",
            "---------------------------------------------------------------\n",
            "\n",
            "XGBClassifier(max_depth=11, objective='multi:softprob') 4  :  0.8866141732283465\n",
            "---------------------------------------------------------------\n",
            "\n",
            "XGBClassifier(n_estimators=214, objective='multi:softprob') 4  :  0.8409448818897638\n",
            "---------------------------------------------------------------\n",
            "\n",
            "XGBClassifier(max_depth=11, n_estimators=214, objective='multi:softprob') 4  :  0.8850393700787401\n",
            "---------------------------------------------------------------\n",
            "\n",
            "KNeighborsClassifier() 5  :  0.8\n",
            "---------------------------------------------------------------\n",
            "\n",
            "KNeighborsClassifier(n_neighbors=1) 5  :  0.8850393700787401\n",
            "---------------------------------------------------------------\n",
            "\n",
            "GaussianNB() 6  :  0.49763779527559054\n",
            "---------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_list=[]\n",
        "sorted_list = sorted(result, key=result.get,reverse=True)\n",
        "\n",
        "for i in sorted_list:\n",
        "  print(i,\"  : \",result[i])\n",
        "  print(\"-------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(sorted_list)\n",
        "\n",
        "\n",
        "flage=[]\n",
        "best_models=[]\n",
        "it=0\n",
        "\n",
        "for i in sorted_list:\n",
        "  if it==4:\n",
        "    break\n",
        "\n",
        "  if i[1] not in flage:\n",
        "    best_models.append((i[0],i[2]))\n",
        "    flage.append(i[1])\n",
        "    it+=1\n"
      ],
      "metadata": {
        "id": "slItYG8uLOFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd69ad9-3c4b-4a82-c4da-1bbaaf810951"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(XGBClassifier(max_depth=11, objective='multi:softprob'), 4, 'xgboost')   :  0.8866141732283465\n",
            "-------------------------------------------------------------------------------------------------\n",
            "(XGBClassifier(max_depth=11, n_estimators=214, objective='multi:softprob'), 4, 'xgboost')   :  0.8850393700787401\n",
            "-------------------------------------------------------------------------------------------------\n",
            "(KNeighborsClassifier(n_neighbors=1), 5, 'KNeighborsClassifier')   :  0.8850393700787401\n",
            "-------------------------------------------------------------------------------------------------\n",
            "(XGBClassifier(n_estimators=214, objective='multi:softprob'), 4, 'xgboost')   :  0.8409448818897638\n",
            "-------------------------------------------------------------------------------------------------\n",
            "(XGBClassifier(objective='multi:softprob'), 4, 'xgboost')   :  0.8\n",
            "-------------------------------------------------------------------------------------------------\n",
            "(KNeighborsClassifier(), 5, 'KNeighborsClassifier')   :  0.8\n",
            "-------------------------------------------------------------------------------------------------\n",
            "(AdaBoostClassifier(random_state=0), 1, 'AdaBoostClassifier')   :  0.5858267716535434\n",
            "-------------------------------------------------------------------------------------------------\n",
            "(GaussianNB(), 6, 'GaussianNB')   :  0.49763779527559054\n",
            "-------------------------------------------------------------------------------------------------\n",
            "[(XGBClassifier(max_depth=11, objective='multi:softprob'), 4, 'xgboost'), (XGBClassifier(max_depth=11, n_estimators=214, objective='multi:softprob'), 4, 'xgboost'), (KNeighborsClassifier(n_neighbors=1), 5, 'KNeighborsClassifier'), (XGBClassifier(n_estimators=214, objective='multi:softprob'), 4, 'xgboost'), (XGBClassifier(objective='multi:softprob'), 4, 'xgboost'), (KNeighborsClassifier(), 5, 'KNeighborsClassifier'), (AdaBoostClassifier(random_state=0), 1, 'AdaBoostClassifier'), (GaussianNB(), 6, 'GaussianNB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best_models:\")\n",
        "for i in best_models:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "id": "SR0XBsJDMi_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d2b302-799f-4489-d632-ba99693e5cbf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_models:\n",
            "(XGBClassifier(max_depth=11, objective='multi:softprob'), 'xgboost')\n",
            "(KNeighborsClassifier(n_neighbors=1), 'KNeighborsClassifier')\n",
            "(AdaBoostClassifier(random_state=0), 'AdaBoostClassifier')\n",
            "(GaussianNB(), 'GaussianNB')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(best_models)"
      ],
      "metadata": {
        "id": "VQCaVYKA5acD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a44b42-f610-4430-c986-0b5ba78473c8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Accuracy For Best 4 Models"
      ],
      "metadata": {
        "id": "znXNx2bgUvtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in best_models:\n",
        "  print(\"--------------------------------------------------\")\n",
        "  print(i[0])\n",
        "  y_pred=i[0].predict(X_train)\n",
        "  print(confusion_matrix(y_train, y_pred))\n",
        "  print(classification_report(y_train,y_pred))\n",
        "  print(\"Accurecy: \",accuracy_score(y_train, y_pred))"
      ],
      "metadata": {
        "id": "2j9CXuBkUuiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38476682-5f0a-4eea-a9c2-e27c52e7ed24"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "XGBClassifier(max_depth=11, objective='multi:softprob')\n",
            "[[630   0   0   0]\n",
            " [  0 648   0   0]\n",
            " [  0   0 625   0]\n",
            " [  0   0   0 634]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       630\n",
            "           1       1.00      1.00      1.00       648\n",
            "           2       1.00      1.00      1.00       625\n",
            "           3       1.00      1.00      1.00       634\n",
            "\n",
            "    accuracy                           1.00      2537\n",
            "   macro avg       1.00      1.00      1.00      2537\n",
            "weighted avg       1.00      1.00      1.00      2537\n",
            "\n",
            "Accurecy:  1.0\n",
            "--------------------------------------------------\n",
            "KNeighborsClassifier(n_neighbors=1)\n",
            "[[630   0   0   0]\n",
            " [  0 648   0   0]\n",
            " [  0   0 625   0]\n",
            " [  0   0   0 634]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       630\n",
            "           1       1.00      1.00      1.00       648\n",
            "           2       1.00      1.00      1.00       625\n",
            "           3       1.00      1.00      1.00       634\n",
            "\n",
            "    accuracy                           1.00      2537\n",
            "   macro avg       1.00      1.00      1.00      2537\n",
            "weighted avg       1.00      1.00      1.00      2537\n",
            "\n",
            "Accurecy:  1.0\n",
            "--------------------------------------------------\n",
            "AdaBoostClassifier(random_state=0)\n",
            "[[414 130  80   6]\n",
            " [220 358  55  15]\n",
            " [131  87 246 161]\n",
            " [  7  16  82 529]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.66      0.59       630\n",
            "           1       0.61      0.55      0.58       648\n",
            "           2       0.53      0.39      0.45       625\n",
            "           3       0.74      0.83      0.79       634\n",
            "\n",
            "    accuracy                           0.61      2537\n",
            "   macro avg       0.60      0.61      0.60      2537\n",
            "weighted avg       0.60      0.61      0.60      2537\n",
            "\n",
            "Accurecy:  0.609775325187229\n",
            "--------------------------------------------------\n",
            "GaussianNB()\n",
            "[[555  21  21  33]\n",
            " [518  60  11  59]\n",
            " [270  22 201 132]\n",
            " [ 96   7  46 485]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.88      0.54       630\n",
            "           1       0.55      0.09      0.16       648\n",
            "           2       0.72      0.32      0.44       625\n",
            "           3       0.68      0.76      0.72       634\n",
            "\n",
            "    accuracy                           0.51      2537\n",
            "   macro avg       0.58      0.52      0.47      2537\n",
            "weighted avg       0.58      0.51      0.46      2537\n",
            "\n",
            "Accurecy:  0.5128104059913283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vdNgE1HYwEjc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ROC"
      ],
      "metadata": {
        "id": "bqG9Vdd0Km6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly==5.11.0\n",
        "!pip install -U kaleido\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ploty_ROC(model,X,y,fig_name):\n",
        "    \n",
        "    lebel_dict={\n",
        "    0: 'Reading',\n",
        "    1: 'Resting',\n",
        "    2: 'Walking',\n",
        "    3: 'Working'\n",
        "    }\n",
        "\n",
        "    y_scores = model.predict_proba(X)\n",
        "\n",
        "    y_onehot = pd.get_dummies(y, columns=model.classes_)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_shape(\n",
        "        type='line', line=dict(dash='dash'),\n",
        "        x0=0, x1=1, y0=0, y1=1\n",
        "    )\n",
        "\n",
        "    for i in range(y_scores.shape[1]):\n",
        "        y_true = y_onehot.iloc[:, i]\n",
        "        y_score = y_scores[:, i]\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "        auc_score = roc_auc_score(y_true, y_score)\n",
        "        name = f\"{lebel_dict[y_onehot.columns[i]]} (AUC={auc_score:.2f})\"\n",
        "        fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title='False Positive Rate',\n",
        "        yaxis_title='True Positive Rate',\n",
        "        yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
        "        xaxis=dict(constrain='domain'),\n",
        "        width=1000, height=1000,\n",
        "        font=dict(\n",
        "            family=\"Courier New, monospace\",\n",
        "            size=20,\n",
        "            color=\"BLack\"\n",
        "        ),\n",
        "        legend=dict(\n",
        "            x=0.67,\n",
        "            y=0.05,\n",
        "            traceorder=\"reversed\",\n",
        "            title_font_family=\"Times New Roman\",\n",
        "            font=dict(\n",
        "                family=\"Courier New, monospace\",\n",
        "                size=20,\n",
        "                color=\"black\"\n",
        "            ),\n",
        "            bgcolor=\"LightSteelBlue\",\n",
        "            bordercolor=\"White\",\n",
        "            borderwidth=2\n",
        "        )\n",
        "    )\n",
        "    fig.show()\n",
        "    fig.write_image(fig_name+\".png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGMhOiOmKqA7",
        "outputId": "cf1b4ea7-ee58-46f4-fd1e-8089e70dd8b5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plotly==5.11.0\n",
            "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly==5.11.0) (8.1.0)\n",
            "Installing collected packages: plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.5.0\n",
            "    Uninstalling plotly-5.5.0:\n",
            "      Successfully uninstalled plotly-5.5.0\n",
            "Successfully installed plotly-5.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9 MB 96 kB/s \n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP"
      ],
      "metadata": {
        "id": "zxxWSX26jsGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "YUhJ7dWajur-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def SHAP_EXP(model,graph_feat):\n",
        "  print(\"Models: \",model)\n",
        "\n",
        "  explainer = shap.Explainer(model.predict, X_test)\n",
        "\n",
        "  shap_values1 = explainer(X_test)\n",
        "  features_names=list_of_feat\n",
        "\n",
        "  if 'Subjects' in features_names:\n",
        "    features_names.pop(0)\n",
        "\n",
        "\n",
        "  shap.plots.bar(shap_values1,max_display=graph_feat[\"max_display\"])\n",
        "\n",
        "  print(\"---------------------\")\n",
        "\n",
        "  shap.summary_plot(shap_values1,max_display=graph_feat[\"max_display\"],feature_names=features_names)\n",
        "\n",
        "  print(\"---------------------\")\n",
        "\n",
        "  print(\"Local Explaination\")\n",
        "  shap.plots.waterfall(shap_values1[graph_feat[\"shap_values Index\"]],max_display=graph_feat[\"max_display\"])\n",
        "\n",
        "\n",
        "  print(\"---------------------\")\n",
        "\n",
        "  shap.plots.bar(shap_values1[graph_feat[\"shap_values Index\"]],max_display=graph_feat[\"max_display\"])"
      ],
      "metadata": {
        "id": "POk0kUrdB-Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_keys_7=models_check_box(models)"
      ],
      "metadata": {
        "id": "V-G5szfQDiz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_feat={\n",
        "    \"max_display\":20,\n",
        "    \"shap_values Index\":2\n",
        "}\n",
        "\n",
        "for i in range(len(new_keys_7)):\n",
        "  if new_keys_7[i].value ==True:\n",
        "    SHAP_EXP(models[i],graph_feat)\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"---------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "JeD8HT_iDodb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LIME**"
      ],
      "metadata": {
        "id": "iDu8Ipdm8qaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "kw4wkx98AuLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import lime\n",
        "from lime import lime_tabular\n",
        "explainer = lime_tabular.LimeTabularExplainer(\n",
        "    training_data=np.array(X_train),\n",
        "    feature_names=list(X.columns),\n",
        "    class_names=['Reading', 'Resting', 'Walking', 'Working'],\n",
        "    mode='classification'\n",
        ")\n",
        "     "
      ],
      "metadata": {
        "id": "KM6UBpC3DBAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = int(input(\"Enter the index of row to explain: \"))      # the index of row to be explained in LIME\n",
        "\n",
        "\n",
        "exp = explainer.explain_instance(X_test.iloc[row],\n",
        "                                 gradBoost_estimator.predict_proba,               #here write the model name\n",
        "                                 num_features=6,\n",
        "                                 top_labels=4)\n",
        "\n",
        "exp.show_in_notebook(show_table=True, show_all=True)\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "html_data = exp.as_html()\n",
        "HTML(data=html_data)\n",
        "\n",
        "\n",
        "exp.save_to_file(\"classif_explanation.html\")"
      ],
      "metadata": {
        "id": "NU96KbYHDIWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "with plt.style.context(\"ggplot\"):\n",
        "    exp.as_pyplot_figure()"
      ],
      "metadata": {
        "id": "u-BkKmWjKGXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ydEeo7l_C1Zs"
      }
    }
  ]
}